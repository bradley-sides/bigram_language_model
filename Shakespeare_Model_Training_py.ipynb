{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC6EVp6zN_iM",
        "outputId": "d691d474-e74d-4f26-e1ee-57f5b6451f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok7fPa-SN5qL",
        "outputId": "a6cb07ca-0acb-46ee-b125-7f6dbb97f201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2841, val loss 4.2815\n",
            "step 500: train loss 2.1700, val loss 2.2206\n",
            "step 1000: train loss 1.7420, val loss 1.8996\n",
            "step 1500: train loss 1.5349, val loss 1.7188\n",
            "step 2000: train loss 1.4206, val loss 1.6385\n",
            "step 2500: train loss 1.3535, val loss 1.5795\n",
            "step 3000: train loss 1.3030, val loss 1.5483\n",
            "step 3500: train loss 1.2626, val loss 1.5143\n",
            "step 4000: train loss 1.2335, val loss 1.5029\n",
            "step 4500: train loss 1.2041, val loss 1.4920\n",
            "\n",
            "Dood so aproams alimason. I am spectary to\n",
            "Let you burn of it? You preversel\n",
            "That he I'll not grant for us herw'd it,\n",
            "Who mine all and gother's employ'd.\n",
            "But, that last'st mal, which were they strike.\n",
            "Which so it book with my hearty, rinan;\n",
            "I had sob a done ere too sigh things bones,\n",
            "All the falseh of hither.\n",
            "O, wife, boy! Therefore the goost fancy, Carl wrong it\n",
            "Is being night dail power: the bishop.\n",
            "Bhisens, I  take not haste, my kingdom,\n",
            "You shall be of and harse thanks, brother,\n",
            "The grave I \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#hyperparameters\n",
        "batch_size = 64 # Num of independent sequences to process in parallel\n",
        "block_size = 256 # Max context length for prediction\n",
        "max_iter = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 384 # batch size * h_head = n_embed\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# -------------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('/content/drive/MyDrive/Shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Unique chars that occur\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# map chars to ints\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # Takes in string, outputs list of ints\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # Takes list of ints, outputs string\n",
        "\n",
        "data = torch.tensor(encode(text), dtype= torch.long)\n",
        "# Split into train and val\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    # Generate small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x,y\n",
        "\n",
        "# Average out loss across multiple batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            lotits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" One head of self-attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.key(x)\n",
        "        # compute attention scores (affinities)\n",
        "        wei = q @ k.transpose(-2, -1) * C ** -0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim = -1)\n",
        "        wei = self.dropout(wei)\n",
        "        # weighted agg of values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self attention in parallel \"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"simple linear layer followed by non-linearity\"\"\"\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        # n_embed = embedding dimension\n",
        "        # n_head = number of heads we want\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off logits for next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed) # Final layer norm\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        B, T = idx.shape\n",
        "        # Predict what's next given a single token\n",
        "        #idx and targets are both (B, T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)#(B, T, C) = batch x time x channel\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) # Stretch array to be 2D\n",
        "            targets = targets.view(B*T) # -1 works too\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # Get predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on last time stop\n",
        "            logits = logits[:, -1, :] # Becomes (B, C)\n",
        "            # Apply softmax to get porbs\n",
        "            probs = F.softmax(logits, dim = -1) # (B, C)\n",
        "            # Sample from the dist\n",
        "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim = 1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for iter in range(max_iter):\n",
        "\n",
        "    # evaluate loss every so often\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#generate from the model\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "\n",
        "print(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNNw_p4vXdrc",
        "outputId": "00dae7e2-b373-42b1-c54c-2c9e141cb3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lord:\n",
            "Soothing Belus, you must withershal: aughous\n",
            "My departs I have see where I think\n",
            "I nor so of inshread my pedposed mine of and your air\n",
            "To save heart, anothersted, when follows false;\n",
            "Forbear the proclaimity of loving does you\n",
            "A frong spar'd and to husband with thee already:\n",
            "Good know now; be come to biding not shrong.\n",
            "\n",
            "ISABELLA:\n",
            "Fith, where\n",
            "I am my good lord, as yet perforced and by charge\n",
            "Than cluelent upon yours' unsteposest made\n",
            "Stand o'er dign either by him; and then 'e,\n",
            "With the viry\n",
            "In a great man: we retues and honour, away:\n",
            "So, now ask it is, sir, you were lineal.\n",
            "\n",
            "Lord Auban;\n",
            "And thus e'en vouchsafest will have been gone,\n",
            "This many could do blew my wife rask;\n",
            "Yet would in, refrend manable other their own:\n",
            "I dare'd the conveyance to tumblush him cries\n",
            "May could be sword as mine eas. I am fairly,\n",
            "Where is the heaven to this virtue deformine,\n",
            "Ure that what stubdue by your brind me in Svinior grieute\n",
            "And I'ld boistements to rather your from my power!\n",
            "Haste the pusse on the heart made and masters:\n",
            "And thy some hathward of mercifus, mother puts!\n",
            "\n",
            "BISHOP OFORD:\n",
            "More, thou mast early is litture to the hotre,\n",
            "Than he think it some more dead, and long to word,\n",
            "From well may ball god: not strive to foot, for the\n",
            "Their own made. Take their breeds, I have my love\n",
            "Shall Hear by force into a naduremer.\n",
            "\n",
            "HENRY:\n",
            "Away! alack her may dearly shame, father mans\n",
            "Cares to stire\n",
            "My armyter: and in my trial will I hund.\n",
            "\n",
            "First Consobrat?\n",
            "The which shall do emistead; and my own meton,\n",
            "They build blains were it argued men;\n",
            "For comes your true, Gaunt left the fallowers\n",
            "Have your bled ungrying the strees: they ay,\n",
            "I swear thee, that shrug adventive of a dired.\n",
            "\n",
            "Lord:\n",
            "Ying adme this woer through if to to have.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Sweet know the country stland of brief?\n",
            "\n",
            "EXTLAND:\n",
            "'Tis note, have witth a falcon's intenced\n",
            "The Volscian of any forbid?\n",
            "\n",
            "CAMILLO:\n",
            "I was an hand. But Madam Cominius.\n",
            "\n",
            "LUCIO:\n",
            "O, by this: marry\n",
            "He hath beened me ofference; Roman's brokes: and\n",
            "My dearly knewly in near of his aCtion abour.\n",
            "\n",
            "LADY ANNE:\n",
            "On peace, he's deliver'd: his heavioner sweet there\n",
            "To fear emptie an emblac: he did no married hand\n",
            "By hands; and himself and mark'd\n",
            "The oddaision and recoinaled right comes\n",
            "To crupt claim: which rights along by the\n",
            "May puts apthority\n",
            "Bolds; as Swelight hand from them.\n",
            "\n",
            "NOLFORK:\n",
            "Mardary Jove it of her troubless;\n",
            "That thus too dewoubt Menenine on Rome,\n",
            "By the body up will war,\n",
            "I proved a king, who yave so rejour they're death.\n",
            "\n",
            "KING RICHARD II:\n",
            "Repeater thou canst like:--a pleasure!- are say.\n",
            "O Romeo shall as be his tongue.\n",
            "\n",
            "QUEN MARGARET:\n",
            "Our name against gebt; that's there.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "On the rast of wit it prosperous Paris!\n",
            "Thy intellaste Hastings court's fellows?\n",
            "\n",
            "QUEN:\n",
            "Thou resolution to thy highness no miscried\n",
            "The best: thought reason, thought'st married whiteous.\n",
            "\n",
            "KING RICHARD III:\n",
            "Then, why, Julius no more lightly.\n",
            "Tell heven alone for tears;\n",
            "Take i the wedsted me frown request's ungriegn!\n",
            "I dre follown this cannot solemn king!\n",
            "Have im thy own deliver fell, the tears,\n",
            "So drink'sy shood York merry shake thy earth;\n",
            "And hear me it, villain to Florbid Rome,\n",
            "Which sperking Henry, tears of Oxford! for all:\n",
            "The gives me call'd me wafte not false,\n",
            "Her straightt, ricele to do the top of \n",
            "From the encounter absence, but on his liife.\n",
            "I thank his name nobately, and slip'd\n",
            "Like to us put thee pumpediment: It prest,\n",
            "Unjusnehold, cousin, and more scoments and from\n",
            "To lowely will purse his parlety tongue.\n",
            "\n",
            "RIVENCES:\n",
            "The unniquired, posts foot that wrans\n",
            "They labour for age\n",
            "Did cemes, laugh do't: say him, if the waters;\n",
            "Being so done, I beseech your mother,\n",
            "All you gues, and GuiltOus frenzed thyself well,\n",
            "Is all nfoding to with outworn Glouction!\n",
            "Methinks' my sovereigngs banishment 'gnodst well night,\n",
            "I not basemers to the cowards my eagle fell\n",
            "To brief our war.\n",
            "A stirrahted morning of flight,\n",
            "Which which sufferits here thus murderers\n",
            "How their frosts-ea-judly'd his heads: and heart thi better\n",
            "At lour chalear meserved it near in the so?\n",
            "So not that he dare-man; her follow to his haste;\n",
            "Or visally ligf upon, but the wild this life\n",
            "Most make town lie this surply fortune of the care\n",
            "And banish'd me as it be, is a banished the\n",
            "duke is. Leave met, 'sta; metre pluck,\n",
            "Come, friends, lets till concealt, let me is be\n",
            "The others: grallamles but of all the glimed mens\n",
            "Tilles to her graces a dog, night\n",
            "As winters shall make gripes himself amove. Hice hence,\n",
            "How lad me at these his offliant, mangost to me;\n",
            "With in heaven like a time death,\n",
            "Or heed, it deny was effected of all the palmer\n",
            "Upon their noble princessaressing shamest smiless tempet,\n",
            "From leave cross nears' goodness; that all thee\n",
            "I by power play and traint Tybalt! God devil,\n",
            "Profeer himself triumpters shall have maided\n",
            "More tenders old this the crown's bearing, so rought!\n",
            "\n",
            "AUTOLYCUS:\n",
            "There's good's; your princess betted for who profers' striple\n",
            "away: hang I'll not sweal\n",
            "I reply as a loseby you.\n",
            "\n",
            "BLUNT:\n",
            "The vile in so rice thrice of scope.\n",
            "\n",
            "ISABELLA:\n",
            "Hear not Ke\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-x9Ns7FXizm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}